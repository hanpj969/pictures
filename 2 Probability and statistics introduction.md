# 2 Probability and statistics introduction  

[TOC]

## Probability basics  



- Sample space ï¼ˆæ ·æœ¬ç©ºé—´â„¦ï¼‰ is the collection of all possible outcomes of a random experiment, often noted as â„¦={ğ}

- In simple coin tossing the sample space is
  > $$
  > â„¦ = \{ğ»ğ‘’ğ‘ğ‘‘ğ‘ , ğ‘‡ğ‘ğ‘–ğ‘™ğ‘ \}
  > $$
  
-  When rolling a fair die the sample space is
  > $$
  > â„¦ = \{1,2,3,4,5,6\}
  > $$
### Eventï¼ˆäº‹ä»¶ï¼‰

- An event is a subset of the sample space to which we will assign a probability

- For the fair die we have, $e.g.$, for the event of  â€oddâ€

  > $$
  > ğ‘ƒ (ğ‘œğ‘‘ğ‘‘) = ğ‘ƒ( \{1,3,5\} )= 0.5
  > $$
### Operations on sets/events

- For two events ğ´ and ğµ we introduce the operations:

- Union ğ´ âˆª ğµ which means that either ğ´ or ğµ or both occur (when we say â€orâ€ we do not mean â€exclusive orâ€)

- Intersection ğ´ âˆ© ğµ which means that ğ´ and ğµ both occur  

- The complement ğ´ğ‘ of an event ğ´ is the event that ğ´ does not occur  

  > ![image-20220507165723733](https://gitee.com/h-ruc/note/raw/master/image-20220507165723733.png)
  >
  > ![image-20220507165733677](https://gitee.com/h-ruc/note/raw/master/image-20220507165733677.png)

### Probability axioms  

- The cornerstones on which probability theory is built
- ğ‘ƒ(ğ´) â‰¥ 0
- ğ‘ƒ(ğ‘†) = 1
- ğ‘ƒ(ğ´âˆªğµ) â‰¤ ğ‘ƒ(ğ´)+ ğ‘ƒ(ğµ) 
- ğ‘ƒ(ğ´^ğ‘^) = 1 - ğ‘ƒ(ğ´)
- ğ‘ƒ (ğ´âˆªğµ) = ğ‘ƒ(ğ´) + ğ‘ƒ(ğµ) - ğ‘ƒ(ğ´âˆ©ğµ)
- If ğ´ is a subset of ğµ then ğ‘ƒ(ğ´) â‰¤ ğ‘ƒ(ğµ)
- Graphically verify these using Venn diagrams!  
### Independent events

- Two events ğ´ and ğµ are said to be independent if  

  > ğ‘ƒ (ğ´ âˆ© ğµ) = ğ‘ƒ(ğ´)ğ‘ƒ(ğµ)  

### Random variables  

- A random variable ğ‘‹ is a mapping from the sample space to the real numbers  

  > ğ‘‹: ğ‘† â†’ â„  
  >
#### Discrete r.v.â€™s

A r.v. ğ‘‹ is said to be discrete if ğ‘ƒ (ğ‘‹ = ğ‘¥~ğ‘–~) > 0 for some list of numbers {ğ‘¥~ğ‘–~ }  

#### Continuous r.v.â€™s  

- A r.v. ğ‘‹ is said to be continuous if it can take any value in an interval (or collection of intervals)

- For a continuous r.v. there exists a (non-negative) density function ğ‘“ such that

  > ğ‘ƒ (ğ‘ â‰¤ ğ‘‹ â‰¤ ğ‘) = $\int_a^b$ğ‘“(ğ‘¥)ğ‘‘ğ‘¥
  > I.e., probabilities are areas under a certain curve   

- ğ‘ƒ(S)=$\int_{-âˆ}^âˆ$ğ‘“(ğ‘¥)ğ‘‘ğ‘¥ = 1

- ğ‘ƒ (ğ‘ < ğ‘‹ < ğ‘) = ğ‘ƒ (ğ‘ â‰¤ ğ‘‹ < ğ‘) = ğ‘ƒ (ğ‘ < ğ‘‹ â‰¤ ğ‘) = ğ‘ƒ (ğ‘ â‰¤ ğ‘‹ â‰¤ ğ‘)

  > since ğ‘ƒ (ğ‘‹ = ğ‘) = ğ‘ƒ (ğ‘‹ = ğ‘) =$\int_{b}^{b}$ğ‘“(ğ‘¥)ğ‘‘ğ‘¥ = $\int_{a}^{a}$ğ‘“(ğ‘¥)ğ‘‘ğ‘¥=0

### Distribution function  

- For a random variable ğ‘‹ we define its distribution function by  

  > ğ¹ (ğ‘¥) = ğ‘ƒ(ğ‘‹ â‰¤ ğ‘¥)  
  >
  > Note that ğ¹(ğ‘¥) â†’ 0 as $ğ‘¥ â†’ -âˆ$, ğ¹(ğ‘¥) â†’ 1 as $ğ‘¥ â†’ âˆ$ and ğ¹(ğ‘¥) â‰¤ ğ¹(ğ‘¦) for ğ‘¥ < ğ‘¦  

#### Distribution function of a discrete r.v  
- For a discrete r.v. ğ‘‹ the distribution function is given by  

  > $F(x)=\sum\limits_{y\le x}P(X=y)$

#### Distribution function of a continuous r.v.  

- For a continuous r.v. ğ‘‹ the distribution function is given by

  > ğ¹(ğ‘¥) = $\int_{-âˆ}^{ğ‘¥}$ğ‘“ (ğ‘¦) ğ‘‘ğ‘¦

- â€The area under the curve from far left up to ğ‘¥â€

  > Note that ğ¹â€² ğ‘¥ = ğ‘“(ğ‘¥)  S

### Expected value

- For a random variable ğ‘‹ we define its Expected value by

  > $E(X)=\int_{-\infty}^{x}xf(x) $

#### Expected value of a discrete r.v.  

- For a discrete r.v. ğ‘‹ the expected value or (theoretical) mean is given by weighting the possible values of ğ‘‹ with its respective probabilities and summing  

  > $ğ¸( ğ‘‹ )= \sum\limits_{ğ‘ğ‘™ğ‘™\ ğ‘¥ }ğ‘¥ğ‘ƒ(ğ‘‹ = ğ‘¥)  $

#### Expected value of a continuous r.v  

- The (theoretical) mean or expected value of continuous r.v. is

  > ğ¸ (ğ‘‹) = $\int_{-âˆ}^{âˆ}$ğ‘¥ğ‘“ (ğ‘¥) ğ‘‘ğ‘¥ 

- For a standard normal ğ‘‹, we have ğ¸ (ğ‘‹) = 0 (verify this) 

###  Computational rule  

- Let ğ‘ and ğ‘ be constants and ğ‘‹ and ğ‘Œ be r.v.â€™s, Then

  > ğ¸ [ğ‘ğ‘‹ + ğ‘ğ‘Œ] = ğ‘ğ¸[ğ‘‹] + ğ‘ğ¸[ğ‘Œ]

- For an arbitrary function ğ‘”, we have  

  > $$
  > E[g(x)]=\left\{ \begin{array}{l}
  > 	\sum_{all\ x}{g\left( xP\left( X=x \right) \right)}\\
  > 	\int_{-\infty}^{\infty}{g\left( x \right) f\left( x \right)}\\
  > \end{array} \right.
  > $$
### Variance and standard deviation

- The variance of an r.v. ğ‘‹ is 

  >  ğ‘‰ğ‘ğ‘Ÿ (ğ‘‹) = ğ¸ [ğ‘‹ - ğ¸[ğ‘‹]^2^]

- So the the variance is a measure of dispersion around the mean

- The standard deviation of ğ‘‹ is the square root of ğ‘‰ğ‘ğ‘Ÿ(ğ‘‹)

  > $ğ‘†ğ·(ğ‘‹) = \sqrt{ğ‘‰ğ‘ğ‘Ÿ(ğ‘‹)} $  

#### Standard deviation  

- Standard deviation is often used as a measure of dispersion since it is expressed in the â€same unitsâ€ as â€the dataâ€
- If the units of â€the dataâ€ are USD the standard deviation is expressed in USD
-  Standard deviation may be used as a (crude) measure of risk of an asset  

#### Variance  

- Computational formula for variance
  ğ‘‰ğ‘ğ‘Ÿ(ğ‘‹) = ğ¸[ğ‘‹^2^] - (ğ¸[ğ‘‹])^2^

  > It is a good exercise to verify this and that **ğ‘‰ğ‘ğ‘Ÿ (ğ‘‹) â‰¥ 0**     ==ALWAYS!== 

#### Variance computational rule  

- If ğ‘ and ğ‘ are constants and ğ‘‹ is a random variable then

  > ğ‘‰ğ‘ğ‘Ÿ(ğ‘+ğ‘ğ‘‹) = ğ‘^2^ğ‘‰ğ‘ğ‘Ÿ(ğ‘‹) 



### Covariance and correlation  

- For two continuous r.v.â€™s ğ‘‹ and ğ‘Œ there may exist a non-negative joint density function ğ‘“ such that  

  > $ğ‘ƒ(ğ‘<ğ‘‹<ğ‘,ğ‘<ğ‘Œ<ğ‘‘)=\int_ğ‘^ğ‘\int_ğ‘^ğ‘‘ğ‘“(ğ‘¥,ğ‘¦)ğ‘‘ğ‘¥ğ‘‘ğ‘¦$

- Two continuous r.v.â€™s ğ‘‹ and ğ‘Œ are said to be independent if the joint density is product of the density of ğ‘‹ and the density of ğ‘Œ

  > ğ‘“(ğ‘¥,ğ‘¦)=ğ‘“~ğ‘‹~(ğ‘¥)ğ‘“~ğ‘Œ~(ğ‘¦)

- So if ğ‘‹ and ğ‘Œ are independent then 

  > ğ‘ƒ( ğ‘<ğ‘‹<ğ‘ , ğ‘<ğ‘Œ<ğ‘‘)=ğ‘ƒ(ğ‘<ğ‘‹<ğ‘)ğ‘ƒ(ğ‘<ğ‘Œ<ğ‘‘)

- The linear dependence between ğ‘‹ and ğ‘Œ may be measured by their covariance

  > ğ¶ğ‘œğ‘£ (ğ‘‹, ğ‘Œ) = ğ¸[ğ‘‹ğ‘Œ] - ğ¸[ğ‘‹]ğ¸[ğ‘Œ]

- For two continuous r.v.â€™s ğ‘‹ and ğ‘Œ

  > $ğ¸[ğ‘‹ğ‘Œ] = \int_{-âˆ}^âˆ\int_{-âˆ}^âˆğ‘¥ğ‘¦ğ‘“( ğ‘¥, ğ‘¦) ğ‘‘ğ‘¥ğ‘‘ğ‘¦$

- Note that if ğ‘‹ and ğ‘Œ are independent then

  > ğ¶ğ‘œğ‘£ (ğ‘‹, ğ‘Œ) = 0  

- However, ğ¶ğ‘œğ‘£ ğ‘‹, ğ‘Œ = 0 does not necessarily imply independence, since covariance only measures linear dependence

- ğ¶ğ‘œğ‘£ (ğ‘‹, ğ‘‹) = ğ‘‰ğ‘ğ‘Ÿ(ğ‘‹)  

- ğ‘‰ğ‘ğ‘Ÿ (ğ‘‹ Â± ğ‘Œ) = ğ‘‰ğ‘ğ‘Ÿ (ğ‘‹) + ğ‘‰ğ‘ğ‘Ÿ(ğ‘Œ) Â± 2ğ¶ğ‘œğ‘£(ğ‘‹, ğ‘Œ)  

  > ==There is NEVER a minus sign between ğ‘‰ğ‘ğ‘Ÿ (ğ‘‹) and ğ‘‰ğ‘ğ‘Ÿ(ğ‘Œ)==

- Note that for independent r.v.â€™s ğ‘‹ and ğ‘Œ

  > ğ‘‰ğ‘ğ‘Ÿ (ğ‘‹ Â± ğ‘Œ) = ğ‘‰ğ‘ğ‘Ÿ ğ‘‹ + ğ‘‰ğ‘ğ‘Ÿ(ğ‘Œ) 

- For independent r.v.â€™s ğ‘‹~1~, â€¦ , ğ‘‹~ğ‘›~ and constants ğ‘~1~, â€¦ , ğ‘~ğ‘›~ we have that

  > $ğ‘‰ğ‘ğ‘Ÿ(\sum\limits_{ğ‘–=1}^{ğ‘›}ğ‘_ğ‘–ğ‘‹_ğ‘– )=\sum\limits_{ğ‘–=1}^{ğ‘›}ğ‘_ğ‘–^2ğ‘‰ğ‘ğ‘Ÿ( ğ‘‹_ğ‘–)$

- The covariance of two r.v.â€™s may be any real number and it is more useful to have measure of linear dependence that takes values in a more narrow interval

- The correlation of ğ‘‹ and ğ‘Œ is given by  

  > $ğ¶ğ‘œğ‘Ÿğ‘Ÿ (ğ‘‹, ğ‘Œ) =\cfrac{ğ¶ğ‘œğ‘£(ğ‘‹, ğ‘Œ)}{\sqrt{ğ‘‰ğ‘ğ‘Ÿ (ğ‘‹) ğ‘‰ğ‘ğ‘Ÿ(ğ‘Œ) }}$

#### Correlation  

- In many books and papers correlation is denoted by ğœŒ (rho) and it can be shown that

  > -1 â‰¤ ğœŒ â‰¤ 1 

- Also if ğœŒ = -1 then Y = ğ‘ğ‘‹ + ğ‘ for ğ‘ < 0

- And if ğœŒ = 1 then Y = ğ‘ğ‘‹ + ğ‘ for ğ‘ > 0  

### Conditional probability/expectation  

- When wanting to express more general (than linear, though including linear) relationships between random variables the notion of conditional expectation is useful  

#### Conditional probabilities for events  

- For two events ğ´ and ğµ the conditional probability of â€ğ´ given ğµâ€ is given by

  > $ğ‘ƒ(ğ´|ğµ) =\cfrac{ğ‘ƒ(ğ´ âˆ© ğµ)}{ğ‘ƒ(ğµ)}$

- We may think of this as reducing the sample space to ğµ

-  **Bayes theorem**  

  >  $ğ‘ƒ(ğ´|ğµ) =\cfrac{ğ‘ƒ(ğµ|ğ´)ğ‘ƒ(ğ´)}{ğ‘ƒ(ğµ)}$

- If ğ´ and ğµ are independent then 

  > ğ‘ƒ (ğ´|ğµ) = ğ‘ƒ(ğ´)  

#### Conditional probabilities for r.v.â€™s

- For discrete r.v.â€™s ğ‘‹ and ğ‘Œ the conditional probability of ğ‘‹ = ğ‘¥ given ğ‘Œ = ğ‘¦ is

  > $ğ‘ƒ (ğ‘‹ = ğ‘¥|ğ‘Œ = ğ‘¦ )= \cfrac{ğ‘ƒ(ğ‘‹=ğ‘¥,ğ‘Œ=ğ‘¦)}{ğ‘ƒ(ğ‘Œ=ğ‘¦)}$
  > where ğ‘‹ = ğ‘¥, ğ‘Œ = ğ‘¦ should be interpreted as
  > \{ğ‘‹ = ğ‘¥\} âˆ© \{ğ‘Œ = ğ‘¦ \}

- For continuous r.v.â€™s ğ‘‹ and ğ‘Œ conditional probabilities are given by introducing the conditional density

  > $ğ‘“ (ğ‘¦|ğ‘¥) =\cfrac
  > {ğ‘“(ğ‘¥, ğ‘¦)}
  > {ğ‘“(ğ‘¥)}$
  > where ğ‘“(ğ‘¥, ğ‘¦) denotes the joint density of ğ‘‹
  > and ğ‘Œ and ğ‘“(x) denotes the density of ğ‘‹  

#### Conditional expectation  

- When trying to establish the relationship between the random variables ğ‘‹ and ğ‘Œ it is convenient to use the conditional expectation ğ¸[ğ‘Œ|ğ‘‹ = ğ‘¥]

- This is a deterministic entity whose value is our â€best guessâ€ of ğ‘Œ given that ğ‘‹ takes the value ğ‘¥  

  ##### If we know the distributions  

  - For discrete ğ‘‹ and ğ‘Œ

    > $ğ¸ [ğ‘Œ| ğ‘‹ = ğ‘¥] = \sum\limits_
    > {ğ‘ğ‘™ğ‘™\ ğ‘¦}
    >  ğ‘¦ğ‘ƒ(ğ‘Œ = ğ‘¦|ğ‘‹ = ğ‘¥)$

  - For continuous ğ‘‹ and ğ‘Œ

    > $ğ¸[ ğ‘Œ |ğ‘‹ = ğ‘¥ ]= \int_
    > {-âˆ}
    > ^âˆ
    > ğ‘¦ ğ‘“( ğ‘¦| ğ‘¥ )ğ‘‘ğ‘¦  $

##### Rules of conditional expectation  

- No matter if we condition on the event â€ğ‘‹ = ğ‘¥â€, the random variable ğ‘‹ or a larger information set â„± the following (and then some, see e.g. Woolridge App. B) rules hold
  - ğ¸ [ğ‘ğ‘Œ + ğ‘ğ‘‹|ğ‘‹ = ğ‘¥] = ğ‘ğ¸ [ğ‘Œ|ğ‘‹ = ğ‘¥] + ğ‘ğ‘¥ or ğ¸ [ğ‘ğ‘Œ + ğ‘ğ‘‹|ğ‘‹] = ğ‘ğ¸ [ğ‘Œ|ğ‘‹] + ğ‘ğ‘‹
    or equivalently if the information generated by ğ‘‹ is contained in â„± then ğ¸ [ğ‘ğ‘Œ + ğ‘ğ‘‹|â„±] = ğ‘ğ¸ [ğ‘Œ|â„±] + ğ‘ğ‘‹
  - If ğ‘Œ is independent of ğ‘‹ or independent of â„± then
    ğ¸ [ğ‘Œ|ğ‘‹ = ğ‘¥] = ğ¸[ğ‘Œ], ğ¸ [ğ‘Œ|ğ‘‹] = ğ¸ [ğ‘Œ] or ğ¸ [ğ‘Œ|â„±] = ğ¸[ğ‘Œ]
  - ğ¸ [ğ¸ [ğ‘Œ|ğ‘‹]] = ğ¸[ğ‘Œ] or ğ¸ [ğ¸ [ğ‘Œ|â„±] ]= ğ¸[ğ‘Œ]  

## In practice  

- In practical statistics or econometrics we often want to estimate population parameters from data

- If we observe a sample ğ‘¥~1~, â€¦ , ğ‘¥~ğ‘›~ from a normal population/r.v. with unknown mean ğœ‡ it is natural to estimate ğœ‡ by

  > $\barğ‘¥ =\frac{1} {ğ‘›}\sum\limits^ğ‘›_{i=1}ğ‘¥_ğ‘–$

- In the same manner we may estimate the variance by
  $\frac{1}{ğ‘› - 1} \sum\limits_{ğ‘–=1}^ğ‘›(ğ‘¥_ğ‘– - \bar ğ‘¥) ^2$
- We get an estimate of the standard deviation if we take the square root in the above expression  
- Also, given pairwise observations (ğ‘¥~1~, ğ‘¦~1~) , â€¦ , (ğ‘¥~ğ‘›~, ğ‘¦~ğ‘›~) we may estimate the covariance of ğ‘‹ and ğ‘Œ by
  $\cfrac{1}{ğ‘› - 1} \left(\sum\limits_{ğ‘–=1}^ğ‘›{ğ‘¥_ğ‘–ğ‘¦_ğ‘–} -\cfrac{\sum_{ğ‘– =1}^{ğ‘›} ğ‘¥_ğ‘– \sum_{ğ‘— =1}^ğ‘› ğ‘¦_ğ‘—}{ğ‘›}\right)$
- The estimated correlation is given by dividing by the product of the estimated stanbard deviations of ğ‘‹ and ğ‘Œ  

## Linear regression  

- Now lets assume that we want to find the straight line that â€fits bestâ€ to our data

- We may denote the line $\hat ğ‘¦ = ğ‘_0 + ğ‘_1ğ‘¥$

- Our wish is to find the â€optimalâ€ constants ğ‘~0~ and ğ‘~1~

- The â€y-coordinateâ€ on the line for a given data point (ğ‘¥~ğ‘–~, ğ‘¦~ğ‘–~) is $\hat ğ‘¦_ğ‘– = ğ‘_0 + ğ‘_1ğ‘¥_ğ‘–$

- We may find the optimal (in the mean square sense) ğ‘~0~ and ğ‘~1~ by minimizing

  > $\sum\limits^{ğ‘›}_{i=1}(ğ‘¦_ğ‘– -\hat ğ‘¦_ğ‘– )^2 = \sum\limits_{ğ‘–=1}
  > ^{ğ‘›}({ğ‘¦_ğ‘– - ğ‘_0 - ğ‘_1ğ‘¥_ğ‘– })^2$
  > w.r.t. ğ‘~0~ and ğ‘~1~  

- A little calculus gives  

  > $$
  > ğ‘_1 =\cfrac
  > {\sum\limits_{ğ‘– =1}^{ğ‘›} {ğ‘¥_ğ‘–ğ‘¦_ğ‘–} âˆ’\cfrac
  > {\sum\limits_{ğ‘– =1}^{ğ‘›} {ğ‘¥_ğ‘–} \sum\limits_{ğ‘— =1}^n ğ‘¦_ğ‘—}
  > {ğ‘›}}
  > {\sum\limits_{ğ‘– =1}^{ğ‘›} ğ‘¥_ğ‘–^2 âˆ’ \cfrac{\left(\sum\limits_{ğ‘– =1}^{ğ‘›} ğ‘¥ğ‘–\right) ^2}{n}}\\
  > ğ‘_0 = \bar ğ‘¦ âˆ’ ğ‘_1\bar ğ‘¥
  > $$

- We may think of the linear regression as a case of conditional expectation

- For this we should consider the response variable ğ‘Œ and the explanatory variable ğ‘Œ as random entitities so that the line is given by

  > ğ‘Œ = ğ›½~0~ + ğ›½~1~ğ‘‹ + ğœ€
  > where ğœ€ is independent of ğ‘‹ and has mean zero  

- The conditional expectation of ğ‘Œ given ğ‘‹ = ğ‘¥ is 

  > ğ¸ [ğ‘Œ|ğ‘‹ = ğ‘¥] = ğ¸ [ğ›½~0~ + ğ›½~1~ğ‘‹ + ğœ€|ğ‘‹ = ğ‘¥]= ğ›½~0~ + ğ›½~1~ğ‘¥

- Then we estimate the intercept ğ›½~0~ and the slope ğ›½~1~ from data using e.g., least squares as above  

- Note that **ğ¸ [ğ‘Œ|ğ‘‹ = ğ‘¥]** is a deterministic entity 

- We may also define conditional expectation as
  a random variable **ğ¸[ğ‘Œ|ğ‘‹]**  

## Prediction of time series

- In time series analysis or econometrics it is often of interest to predict future values of the series {ğ‘‹~ğ‘¡~}~ğ‘¡â‰¥0~ using the information available up until the time when the predicition is made
- In this case, letting â„±~ğ‘¡~ denote the information available at time ğ‘¡, our best guess (in the mean square sense) of ğ‘‹~ğ‘¡+â„~ is ğ¸ [ğ‘‹~ğ‘¡+â„~| â„±~ğ‘¡~]  

## Statics

### The most important distribution  

- A (continuous) r.v. ğ‘ is said to have standard normal distribution if its density is  

 > $$
 > f\left( x \right) =\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}},\;x\in \left( -\infty ,\;+\infty \right)
 > $$
 >
 > ![image-20220507210046916](https://gitee.com/h-ruc/note/raw/master/image-20220507210046916.png)

- The probability that ğ‘ â‰¤ ğ‘§ is the area under the â€bell shapeâ€ from â€far leftâ€ to ğ‘§  
- It should be noted that for a standard normal r.v. ğ‘ probabilities are not possible to â€calculate by handâ€ since there does not exist a primitive function for the density 
- In many books the distribution function of a standard normal r.v. is denoted
  $Î¦(ğ‘§) = ğ‘ƒ(ğ‘ â‰¤ ğ‘§) =\frac{1}{\sqrt{2ğœ‹}}\int_{-âˆ}{ğ‘§}ğ‘’^{-\frac{ğ‘¥^2}{2}} ğ‘‘ğ‘¥$
- Values of $Î¦$ for different values of ğ‘§ are found in tables or using numerics  

#### Symmetries  

- Since the â€bell shapeâ€ is symmetric it follows that

  > $Î¦ (-ğ‘§) = 1 - Î¦ (ğ‘§)$ 

- Verify this by drawing some â€filled bell shapesâ€ for different values of ğ‘§  

#### Scale and location

- A (continuous) r.v. ğ‘‹ is said to have normal distribution with mean ğœ‡ and standard deviation ğœ if 

  > ğ‘‹ = ğœ‡ + ğœğ‘
  > where ğ‘ is standard normal  

#### Standardizing a normal r.v  

- If ğ‘‹ is normal with mean ğœ‡ and standard deviation ğœ then $ğ‘ =\frac{ğ‘‹ - ğœ‡}{ğœ}$ is standard normal  

#### Normal r.v.â€™s  

- The standard deviation of a standard normal r.v. is 1 (good exercise in integrating by parts)
- For a normal r.v. ğ‘‹ = ğœ‡ + ğœğ‘, where ğ‘ is standard normal, we have (verify this)
  ğ¸ [ğ‘‹] = ğœ‡ and  ğ‘‰ğ‘ğ‘Ÿ (ğ‘‹) = ğœ2 and thus ğ‘†ğ· ğ‘‹ = ğœ  

#### Why is the normal distribution so important? (CGS)  

- Starting from a sample, i.e., a collection of independent and identically distributed r.v.â€™s ğ‘‹~1~, â€¦ , ğ‘‹~ğ‘›~, with mean ğœ‡ and standard deviation ğœ it can be shown that the distribution of $\cfrac{\frac{1}{ğ‘›}\sum\limits_{ğ‘–=1} ^ğ‘› ğ‘‹_ğ‘– - ğœ‡}{ğœ\sqrt ğ‘›}$ approaches the standard normal as the sample size ğ‘› increases  

### Some more statistics  

- In statistics the normal distribution is highly important since many statistical methods require normally, or at least approximately normally, distributed data
- There are also some important distributions derived from the normal upon which some important statistical methods are built

####   Some distributions derived from the normal  

- The $\chi^2$ (chi-square) distribution with ğ‘› degrees off reedom is what you get if you add ğ‘› squaredi ndependent standard normals

  > $ğ‘‹ = \sum\limits_{ğ‘–=1}^{ğ‘›} ğ‘_ğ‘–^2$

  - This distribution is usually present when trying to estimate variances for normal popoulations  

- The (Student) ğ‘¡ distribution is what you get if you let

  > $ğ‘‡ =\cfrac{ğ‘}{\sqrt{ğ‘‹/ğ‘›}}$
  > where ğ‘ is standard normal and ğ‘‹ is $Ï‡^2$ with ğ‘› degrees of freedom and independent of ğ‘

  - This distribution is usually present when trying to estimate or compare means of normal populations for which the variance is unknown  

- The ğ¹ distribution with ğ‘˜ and ğ‘™ degrees of freedom is what you get if you define

  > $ğ¹ =\cfrac{ğ‘‹_ğ‘˜/ğ‘˜}{ğ‘‹_ğ‘™/ğ‘™}$
  > where ğ‘‹~ğ‘˜~ and ğ‘‹~ğ‘™~ are independent â€chi-squaresâ€ with ğ‘˜ and ğ‘™ degrees of freedom, respectively

  - This distribution is usually present when trying to compare
    variances of two independent normal populations  

### Central Limit Theorem  

- if you start with a sample from ==ANY== distribution (with well defined mean
  and std.dev) the sample mean $\barğ‘‹ = \frac{1}{ğ‘›}\sum\limits_{ğ‘–=1}^ ğ‘› ğ‘‹_ğ‘–$ has an approximate normal distribution  

### Point and interval estimates  

- In statistics it is often of interest two estimate population parameters and above we have seen some examples of point estimates for means, variances, covariances, slopes etc...
- To get better control of how precise your point estimates are it is useful to introduce confidence intervals  

### Confidence intervals  

- We say that a (1 - ğ›¼) âˆ™ 100% confidence interval for the population parameter ğœƒ is a random interval [ğ¿, ğ‘ˆ] such that

  > ğ‘ƒ (ğ¿ â‰¤ ğœƒ â‰¤ ğ‘ˆ) = 1 - ğ›¼  

- For a normal population with known variance ğœ^2^ a 95% c.i. for the **(unknown)** mean ğœ‡, based on the sample ğ‘‹~1~, â€¦ , ğ‘‹~ğ‘›~, is 

  > $ğ‘ƒ (\barğ‘‹ - 1.96 ğœ/\sqrt ğ‘› â‰¤ ğœ‡ â‰¤ \barğ‘‹ - 1.96 ğœ\sqrt ğ‘›) = 0.95$

- The observed confidence interval in the above case is

  > $\barğ‘¥ - 1.96 ğœ/\sqrt ğ‘› , \barğ‘¥ - 1.96 ğœ/\sqrt ğ‘›$

- It should be noted that at this point, i.e., once the confidence interval is realized it is pointless to think that it cointains the true value of ğœ‡ with 95% probability since there is â€nothing random leftâ€ (all quantities comprising the interval are deterministic and the true, though unknown but deterministic, parameter value is either inside or outside the interval)

- When interpreting realized confidence intervals we should think; If we create a whole lot of 95% confidence intervals for a large bunch of different parameters, 95% of them will contain the true parameter values  

### Hypothesis testing  

- Related to parameter estimation is hypothesis testing for parameters

- Hypothesis testing may also concern more â€advanced mattersâ€ such as â€does the data at hand come from a normal distributionâ€  

  #### Hypothesis testing in general  

  - First we set up som hypotheses

    > ğ»~0~: ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’ğ‘šğ‘’ğ‘›ğ‘¡ ğ‘¡ğ‘Ÿğ‘¢ğ‘’
    > ğ»~1~: ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’ğ‘šğ‘’ğ‘›ğ‘¡ ğ‘“ğ‘ğ‘™ğ‘ ğ‘’

  - ğ»~0~ is called the null hypothesis

  - ğ»~0~ is called the alternative hypothesis  

  - Based on sample data from a population we should decide whether to reject ğ»~0~ or not

  - Calculations are made under the assumption of ğ»~0~ being true

  - If the results of theses calculations turn out to be sufficiently unlikely ğ»~0~ is rejected  

  #### Possible errors  

  > <img src="https://gitee.com/h-ruc/note/raw/master/image-20220508163239043.png" alt="image-20220508163239043" style="zoom:50%;" />
  >
  > 

#### 					Scheme  

- Set up hypotheses
- Decide on probability of Type I error
- Decide on test statistic and decision rule
- Given sample data, make computations under ğ»~0~
- Decide wether to reject or not reject ğ»~0~
- Interpret and find â€business implicationsâ€ of your decision  

### p-value  

- p-value means that, under the null hypothesis, the probability that **the test statistic takes a value at least as extreme as the value at hand** is p
- In other words **a low p-value** means it is **unlikely** that **the data at hand comes from a population specified by the null hypothesis**
- We reject the null hypothesis if we get a p-value that is lower than ğ›¼  