# 3 Simple Linear Regression

[TOC]  

## What is regression analysis?  

Regression analysis is concerned with the study of the dependence of one vari-able, the dependent variable, on one or more other variables, the explanatory vari-ables, with a view to estimating and/or predicting the (population) mean or aver-age value of the former in terms of the known or fixed (in repeated sampling) values of the latter. 

## Some Terminologyï¼ˆæœ¯è¯­ï¼‰  

### In the simple linear regression model, ==$y = \beta_0 + \beta_1x + u$==  

- We typically refer to y as the
  - Dependent Variable, or
  - Left-Hand Side Variable, or
  - Explained Variable, or
  - Regressand
- We typically refer to x as the
  - Independent Variable, or
  - Right-Hand Side Variable, or
  - Explanatory Variable, or
  - Regressor, or
  - Covariate, or
  - Control Variables  

> ![image-20220508173450193](https://gitee.com/h-ruc/note/raw/master/image-20220508173450193.png)

- Error term u contains ï¼š

  - Effects of other factors

    > e.g. habit, food structure

  - Measurement error æµ‹é‡è¯¯å·®

    > e.g. calculation mistake

## A Simple Assumption: Mean Error Term  

- The average value of u, the error term, in the population is 0. That is, $E(u) = 0$

  > Note: this is not a restrictive assumption, since we can always use b~0~ to normalize $E(u)$ to 0  

## Zero Conditional Mean  

- We need to make a crucial assumption about how u and x are related: we want it to be the case that knowing something about x does not give us any information about u, so that they are completely unrelated, i.e. $E(u|x)=0$.
- $E(u|x)$ implies $E(y|x)$ = b~0~ + b~1~x 
- When $E(u|x) =0$, we have $E(u) = 0  $

## Ordinary Least Squares ï¼ˆæ™®é€šæœ€å°äºŒä¹˜æ³•ï¼‰  

- The basic idea of regression is to estimate **the population parameters** from a <u>sample.</u>

- Let $\{(xi,yi): i=1, â€¦,n\}$ denote a random sample of size n from the
  population that satisfifies $y = \beta_0 + \beta_1x + u$

- Then, for each observation i (ä»»æ„ä¸€ä¸ªè§‚æµ‹å€¼i) in this sample,it will be the case that $y_i = \beta_0 + \beta_1x_i + u_i$

- Population regression line, data points and the associated error termsf  

  > ![image-20220508175618536](https://gitee.com/h-ruc/note/raw/master/image-20220508175618536.png)

- $E(y|x)$ as a linear function of x, where for any x, the distrihution of y is centered about $E(y|x) $

  > ![image-20220508180306002](https://gitee.com/h-ruc/note/raw/master/image-20220508180306002.png)

### Population regression function/line  

> ![image-20220508180813523](https://gitee.com/h-ruc/note/raw/master/image-20220508180813523.png)

- Regression evaluates numerical relation between economic
  variables.

  - Linear regressionï¼š assuming population parameters are linear

  - Simple regressionï¼š relation of one variable and the other variable

    > $y_i = E(y | x_i ) + u_i = \beta_0 + \beta_1x_i + u_i$  

    - ğ‘¦~ğ‘–~: dependent variable(å› å˜é‡)
    - ğ‘¥~ğ‘–~:independent variable(è‡ªå˜é‡ )
    - ğ›½~0~: Intercept(æˆªè·é¡¹)
    - ğ›½~1~: Slope(æ–œç‡ )
    - ğ‘¢~ğ‘–~: error term(è¯¯å·®é¡¹)

### Sample regression function (SRF)  

- The sample counterpart of PRF can be written as:

  > $ğ‘Œ_ğ‘– = \hat ğ›½_0 + \hat ğ›½_1 ğ‘‹_ğ‘– + \hat ğ‘¢_ğ‘– = \hat ğ‘Œ_i + \hat ğ‘¢_ğ‘–$

  - where $\hat ğ‘Œ$ is read as "Y-hat" or "Y-cap" ,$\hat ğ‘Œ_i$ = estimator of $E(Y | X_i) $
  - $\hatğ›½_0$: the estimator of population parameter ğ›½~0~ï¼ˆæ€»ä½“å‚æ•°ğ›½~0~ çš„ä¼°è®¡å€¼ï¼‰
  - $\hatğ›½_1$: the estimator of population parameter ğ›½~1~ï¼ˆæ€»ä½“å‚æ•°ğ›½~1~ çš„ä¼°è®¡å€¼ï¼‰

  > ![image-20220508181848631](https://gitee.com/h-ruc/note/raw/master/image-20220508181848631.png)

### Deriving M.O.M. ï¼ˆçŸ©ä¼°è®¡ï¼‰ Estimates  

- The method of moments ï¼ˆ M.O.M.ï¼‰ approach to estimation implies imposing the population moment restrictions on the sample moments

- To derive the M.O.M. estimates we need to realize that our main assumption of $E(u|x) = E(u) = 0$ also implies that $Cov(x,u) = E(xu) = 0$

  > ==Note:==
  >
  > - Remember from basic probability that $Cov(X,Y) = E(XY) â€“ E(X)E(Y)$
  > - What does $E(u) =E(y â€“ \beta_0 â€“ \beta_1x) = 0$ mean? Recall that for E(X), the mean of a population distribution, a sample estimator of E(X) is simply the arithmetic mean of the sample  

- We can write our 2 restrictions just in terms of x, y, $\beta_0$ and $\beta_1$ ,since $u = y â€“ \beta_0 â€“ \beta_1x$

  > **==population moment restrictions==**  

  - $E(u)=E(y â€“ \beta_0 â€“ \beta_1x) = 0$
  - $E(xu)=E[x(y â€“ \beta_0 â€“ \beta_1x)] = 0$

- Based on population moment restrictions, we find sample analogy as follows, using <u>the law of large samples</u>: 

  > **==Sample analogy==**  
  > $$
  >  \begin{align}
  >  &\sum(ğ‘¦_ğ‘– âˆ’ ğ‘_0 âˆ’ ğ‘_1ğ‘¥_ğ‘–) = 0 \\
  >  &\sumğ‘¥_ğ‘–(ğ‘¦_ğ‘– âˆ’ ğ‘_0 âˆ’ ğ‘_1ğ‘¥_ğ‘–) = 0
  >  \end{align}
  > $$
  >  **æ³¨**ï¼šè¿™é‡Œéšå«äº†$nâ†’ âˆ$æ—¶ï¼Œæ ·æœ¬å‡å€¼æ‰ä¾æ¦‚ç‡æ”¶æ•›äºæ€»ä½“çš„å‡å€¼ã€‚å› æ­¤ï¼Œ nçš„æ•°é‡å¿…é¡»ä¸èƒ½å°äºä¸€å®šçš„æ•°ï¼ˆé€šå¸¸è‡³å°‘ä¸èƒ½å°äº30ï¼‰ .  

### Ordinary Least Squared (OLS) Estimation  

- Assume we have data on income and expenditure, how to calculate Î²~0~ and Î²~1~ ï¼Ÿ

- OLS calculates b~0~, b~1~ , estimation of Î²~0~,Î²~1~ by minimizing sum of squared residuals

  - Residual: $\hat ğ‘¢_ğ‘– = ğ‘¦_ğ‘– - ğ‘_0 - ğ‘_1ğ‘¥_ğ‘–$

  - Minimize sum of squared residuals

    > $\min \limits_{ğ‘_0,ğ‘_1} \sum\limits_{ğ‘–=1}^{ğ‘›}(ğ‘¦_ğ‘– - ğ‘_0 - ğ‘_1ğ‘¥_ğ‘–)^2$

  - $$
    \begin{align}
    &\cfrac{ğœ•}
    {ğœ•ğ‘_0} = -2 \sum(ğ‘¦_ğ‘– - ğ‘_0 - ğ‘_1ğ‘¥_ğ‘–) = -2 \sum \hatğ‘¢_ğ‘– = 0 \\
    &\cfrac{ğœ•}
    {ğœ•ğ‘_1} = -2 \sum ğ‘¥_ğ‘–(ğ‘¦_ğ‘– - ğ‘_0 - ğ‘_1ğ‘¥_ğ‘–) = -2 \sum ğ‘¥_ğ‘–\hat ğ‘¢_ğ‘– = 0 
    \end{align}
    $$

#### Calculate b~1~

- $$
  \begin{align}
  &\bar{y}-b_00-b_1\bar x=0â‡’ ğ‘_0 = ğ‘¦ âˆ’ ğ‘_1ğ‘¥\\
  &\sum ğ‘¥_ğ‘–[ğ‘¦_ğ‘– âˆ’ ( ğ‘¦ âˆ’ ğ‘_1ğ‘¥) âˆ’ ğ‘_1ğ‘¥_ğ‘–] = 0\\
  &\sum ğ‘¥_ğ‘–[(ğ‘¦_ğ‘– âˆ’ \bar ğ‘¦) âˆ’ ğ‘_1(ğ‘¥_ğ‘– âˆ’ \bar ğ‘¥)] = 0\\
  &\sum ğ‘¥_ğ‘–(ğ‘¦_ğ‘– âˆ’ \bar ğ‘¦) âˆ’ ğ‘_1 \sum ğ‘¥_ğ‘–(ğ‘¥_ğ‘– âˆ’ \bar ğ‘¥) = 0\\
  &ğ‘_1 =
  \sum \cfrac{ğ‘¥_ğ‘–(ğ‘¦_ğ‘– âˆ’ \bar ğ‘¦)}
  {\sum ğ‘¥_ğ‘–(ğ‘¥_ğ‘– âˆ’ \barğ‘¥)} =\cfrac
  {\sum(ğ‘¥_ğ‘– âˆ’ \barğ‘¥)(ğ‘¦_ğ‘– âˆ’ \barğ‘¦)}
  {\sum(ğ‘¥_ğ‘– âˆ’ \barğ‘¥)^2}
  \end{align}
  $$

#### Algebraic Properties of OLS Estimation  

- $\cfrac{\partial}{\partial b_0}=-2\sum (y_i-b_0-b_1x_1)=-2\sum\hat u_i=0$

- $\cfracğœ• {ğœ•ğ‘_1} = -2 \sum ğ‘¥_ğ‘–(ğ‘¦_ğ‘– - ğ‘_0 - ğ‘_1ğ‘¥_ğ‘–) = -2 \sum ğ‘¥_ğ‘–\hat ğ‘¢_ğ‘– = 0   $

- $\bar ğ‘¦ - ğ‘_0 - ğ‘_1\bar ğ‘¥ = 0   â‡’ \bar ğ‘¦ = ğ‘_0 + ğ‘_1 \bar x $

- ==æ³¨ï¼šæ— è®ºE(u)=0å’ŒE(xu)=0è¿™ä¸¤ä¸ªæ¡ä»¶æ˜¯å¦æˆç«‹ï¼Œä»¥ä¸Šä¸‰æ¡æ°¸è¿œæˆç«‹ã€‚åŸ
  å› ï¼šå®ƒä»¬æ˜¯ä»OLSçš„ä¼˜åŒ–è¿‡ç¨‹ä¸­æ¨å¯¼å‡ºæ¥çš„ä»£æ•°æ€§è´¨ã€‚==

  > ![image-20220511211802513](https://gitee.com/h-ruc/note/raw/master/image-20220511211802513.png)

#### When $nâ†’âˆ$  

- $\lim\limits_{ğ‘›â†’âˆ}ğ‘_1 =\cfrac{ğ¶ğ‘œğ‘£(ğ‘¥_ğ‘–, ğ‘¦_ğ‘–)}{ğ‘‰ğ‘ğ‘Ÿ(ğ‘¥_ğ‘–)}  $

### æ‹Ÿåˆä¼˜åº¦  

- $ğ‘¦_ğ‘– = \hat ğ‘¦_ğ‘– + \hat ğ‘¢_ğ‘–$

- $\hat y_ğ‘– = ğ‘_0 + ğ‘_1ğ‘¥_ğ‘–$, is explained part

- $\hat u_ğ‘– = ğ‘¦_ğ‘– - \hat ğ‘¦_ğ‘–$, is unexplained partà·

-  $\sum (ğ‘¦_ğ‘– - \barğ‘¦)^ 2$ is the total sum of squares **(SST)** æ€»å¹³æ–¹å’Œ

-  $\sum(\hat y_ğ‘– - \bar ğ‘¦)^ 2$ is the explained sum of squares **(SSE)** è§£é‡Šå¹³æ–¹å’Œ

-  $\sum \hatğ‘¢_ğ‘–^2$ is the residual sum of squares **(SSR)** æ®‹å·®å¹³æ–¹å’Œ

- Then **SST = SSE + SSR**

  > $R^2 = \cfrac{ğ‘†ğ‘†ğ¸}{ğ‘†ğ‘†ğ‘‡} = 1 -\cfrac{ğ‘†ğ‘†ğ‘…}{ğ‘†ğ‘†ğ‘‡} $ evaluates how well SST in ğ‘¦~ğ‘–~ is explained by ğ‘¥~ğ‘–~
  >
  > ![image-20220512223916542](https://gitee.com/h-ruc/note/raw/master/image-20220512223916542.png)

## Assumptions underlying the OLS method  

- Our goal is not only to obtain the estimated values of $\beta_0,\beta_1$, but also to draw inferences about the true.In other words, we need **unbiased estimators**. To this end, we need to make some assumptions for OLS method. This is often known as **==â€œGauss-Markov assumptionsâ€==**

  ### Assumptions for unbiasedness of SLR  

  - SLR 1ï¼š population model is linear in parameters

    > $y=\beta_0+\beta_1x+u$

  - SLR 2ï¼š random sampling

    > $Cov({X_i,X_j})=Cov({u_i,u_j})=0$

  - SLR 3ï¼š there are variations in X

    > $\sum\limits_{i=1}^n(x_i-\bar x)^2\ne0$

  - SLR 4ï¼š Zero conditional mean $E(u|x)=0$

    > b.c. random sampling, $E(u_i|x_i)=0$
    > â€“ aï¼š Zero mean E(u)=0
    > â€“ bï¼š uncorrelated E(ux)=0
    >
    > ğ‘‚ğ¿ğ‘†çš„æ•°å€¼ç‰¹å¾
    > $\sum\hat ğ‘¢_ğ‘– = 0$
    > $\sum\hat ğ‘¢_ğ‘–ğ‘¥_ğ‘– = 0  $

  
  ### 		Homoscedasticity assumption  
  
  - SLR 5: homoscedasticity $Var(u|x)= Ïƒ^2$
  
    > $$
    > \begin{align}
    > &ğ‘‰ğ‘ğ‘Ÿ(ğ‘¢|ğ‘¥) = E(ğ‘¢^2|ğ‘¥) - [E(ğ‘¢|ğ‘¥)] ^2\\
    > &b.c. E(ğ‘¢|ğ‘¥) = 0ï¼Œ ğ‘‰ğ‘ğ‘Ÿ(ğ‘¢|ğ‘¥) = ğœ^2\\
    > &so E(ğ‘¢^2|ğ‘¥) = ğœ^2\\
    > &so E(ğ‘¢^2) = ğœ^2
    > 
    > \end{align}
    > $$
  
  - 